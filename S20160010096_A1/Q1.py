# -*- coding: utf-8 -*-
"""DL - A1 - Q1 - XOR MLP_V1_Relu_Sigmoid.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RosLu9jVQJTMz5A_Cb_6UuXyCEwYPrRH
"""

"""
  XOR - MLP CLASSIFIER

  Name - Tanmay Kalani
  Roll No. - S201600100096
"""

import numpy as np
import matplotlib.pyplot as plt

NUMBER_INPUT_UNITS = 2
NUMBER_HIDDEN_UNITS = 2
NUMBER_OUTPUT_UNITS = 1

LEARNING_RATE = 0.009
NUMBER_OF_TRAINING_EXAMPLES = 4
EPOCHS = 10000

cost = np.zeros((EPOCHS, 1))

def sigmoid(z):
  return 1 / (1 + np.exp(-z))

def sigmoid_derv(z):
  return z * (1 - z)

def relu(z):
    return np.maximum(z, 0)

def reluDerivative(x):
    return np.greater(x, 0).astype(int)

def train(W1, W2, B1, B2):
  for i in range(0, EPOCHS):
    c, diff_W1, diff_W2, diff_B1, diff_B2 = 0, 0, 0, 0, 0
    for j in range(NUMBER_OF_TRAINING_EXAMPLES):
      
      # FORWARD PROPAGATION
      a0 = X[j].reshape(X[j].shape[0], 1)
      
      z1 = W1.dot(a0) + B1          # Z1 = INPUT * WEIGHTS + BIAS
      a1 = relu(z1)                 # A1 = activation(Z1)
      
      z2 = W2.dot(a1) + B2          # Z2 = A1 * WEIGHTS + BIAS
      a2 = sigmoid(z2)              # A2 = activation(Z2)
      
      # BACKPROPAGATION
      dz2 = a2 - y[j]               # ERROR = OUTPUT_VALUE - EXPECTED_VALUE
      diff_W2 += dz2 * a1.T         # Backprop - Output-Hidden layer
      
      dz1 = np.multiply((W2.T * dz2), reluDerivative(a1))
      diff_W1 = dz1.dot(a0.T)       # Backprop - Hidden-Input layer
      
      diff_B1 += dz1                # Updating Bias
      diff_B2 += dz2                # Updating Bias
      
      c = c + (-(y[j] * np.log(a2)) - ((1 - y[j]) * np.log(1 - a2)))      # Cost after each epoch

      W1 -= LEARNING_RATE * (diff_W1)     # UPDATING INPUT-HIDDEN LAYER WEIGHTS
      W2 -= LEARNING_RATE * (diff_W2)     # UPDATING HIDDEN-OUTPUT LAYER WEIGHTS
      B1 -= LEARNING_RATE * (diff_B1)     # UPDATING INPUT LAYER BIAS WEIGHT
      B2 -= LEARNING_RATE * (diff_B2)     # UPDATING HIDDEN LAYER BIAS WEIGHT

    cost[i] = c / NUMBER_OF_TRAINING_EXAMPLES     # COST
    print('Cost after epoch - {} : {}'.format(i+1, cost[i]))

  return (W1, W2, B1, B2)

def test(W1, W2, B1, B2):
  for j in range(NUMBER_OF_TRAINING_EXAMPLES):
    a0 = X[j].reshape(X[j].shape[0], 1)
    z1 = W1.dot(a0) + B1            # Z1 = INPUT * WEIGHTS + BIAS
    a1 = relu(z1)                   # A1 = activation(Z1)

    z2 = W2.dot(a1) + B2            # Z2 = A1 * WEIGHTS + BIAS
    a2 = sigmoid(z2)                # A2 = activation(Z2)
    
    print(a2)

if __name__ == '__main__':
  X = np.array([                                    # FEATURE VECTORS
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
  ])

  y = np.array([                                    # EXPECTED OUTPUTS
      [0],
      [1],
      [1],
      [0]
  ])

  print('**'*25)
  print('X = {}'.format(X))
  print('y = {}'.format(y))

  np.random.seed(2016)

  W1 = np.random.normal(0, 1, (NUMBER_HIDDEN_UNITS, NUMBER_INPUT_UNITS))      # INITIALIZING INPUT-HIDDEN LAYER WEIGHTS ( N(0, 1) )
  W2 = np.random.normal(0, 1, (NUMBER_OUTPUT_UNITS, NUMBER_HIDDEN_UNITS))     # INITIALIZING HIDDEN-OUTPUT LAYER WEIGHTS ( N(0, 1) )

  B1 = np.random.random((NUMBER_HIDDEN_UNITS, 1))                             # INITIALIZING INPUT-HIDDEN LAYER BIAS ( N(0, 1) )
  B2 = np.random.random((NUMBER_OUTPUT_UNITS, 1))                             # INITIALIZING INPUT-HIDDEN LAYER BIAS ( N(0, 1) )

  # SHAPES OF WEIGHT MATRICES
  print('{} - SHAPE OF WEIGHT MATRICES - {}'.format('*'*25, '*'*25))
  print('W1 shape = {}'.format(W1))
  print('W2 shape = {}'.format(W2))
  print('B1 shape = {}'.format(B1))
  print('B2 shape = {}'.format(B2))

  # INITIAL RANDOM WEIGHTS
  print('{} - INITIAL WEIGHTS - {}'.format('*'*25, '*'*25))
  print('W1 = {}'.format(W1))
  print('W2 = {}'.format(W2))
  print('B1 = {}'.format(B1))
  print('B2 = {}'.format(B2))

  # TRAINING
  print('{} - TRAINING - {}'.format('*'*25, '*'*25))
  W1, W2, B1, B2 = train(W1, W2, B1, B2)

  # FINAL WEIGHTS
  print('{} - FINAL WEIGHTS - {}'.format('*'*25, '*'*25))
  print('W1 = {}'.format(W1))
  print('W2 = {}'.format(W2))
  print('B1 = {}'.format(B1))
  print('B2 = {}'.format(B2))

  # COST PLOT
  plt.plot(range(EPOCHS), cost)
  plt.xlabel("Iterations")
  plt.ylabel("Cost")
  plt.show()

  # TEST
  print('{} - TESTING - {}'.format('*'*25, '*'*25))
  test(W1, W2, B1, B2)