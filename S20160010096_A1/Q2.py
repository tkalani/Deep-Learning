# -*- coding: utf-8 -*-
"""DL_A1_Q2_CNN_V2_Coloured_100_0_009.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m1LNGEtVvKazlKvNeMcD8bdcjr282k2w
"""

"""
  CNN

  Name - Tanmay Kalani
  Roll No. - S20160010096
"""

import math
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import tensorflow as tf
from tensorflow.python.framework import ops
import os, cv2

# from google.colab import drive
# drive.mount('/content/drive')


def convert_to_one_hot(Y, C):
  """
    Convert to one-hot-encoding
  """
  Y = np.eye(C)[Y.reshape(-1)].T
  return Y

def randomize_and_load_dataset(X, Y):
  """
    Randomize and Load Dataset
  """
  X_train_orig, Y_train_orig, X_test_orig, Y_test_orig = np.array([]), np.array([]), np.array([]), np.array([])       # Define np-arrays
  classes = np.array([1, 2, 3, 4, 5, 6, 7])                                                                           # Define class array
  for i in range(X.shape[0]):
    if i % 4 != 0:                                                                                           # Put every 4th instance to test ( Test Ratio = 0.25 )
      if X_train_orig.size == 0:
        X_train_orig = np.array([X[i]])
        Y_train_orig = np.array([[Y[0][i]]])
      else:
        X_train_orig = np.append(X_train_orig, [X[i]], axis=0)
        Y_train_orig = np.append(Y_train_orig, [[Y[0][i]]], axis=1)
    else:
      if X_test_orig.size == 0:
        X_test_orig = np.array([X[i]])
        Y_test_orig = np.array([[Y[0][i]]])
      else:
        X_test_orig = np.append(X_test_orig, [X[i]], axis=0)
        Y_test_orig = np.append(Y_test_orig, [[Y[0][i]]], axis=1)
  return X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes

def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):
  """
    Creates minibatches for schochatic Adam Optimization

    Args :
      X - input feature vectors
      Y - output feature vectors

    Returns:
      mini_batch - list of minibatches
  """
  m = X.shape[0]                  # number of training examples
  mini_batches = []
  np.random.seed(seed)
  
  # Shuffle X and Y
  permutation = list(np.random.permutation(m))
  shuffled_X = X[permutation,:,:,:]
  shuffled_Y = Y[permutation,:]

  # Partition shuffled_X and shuffled_Y
  num_complete_minibatches = math.floor(m/mini_batch_size)
  for k in range(0, num_complete_minibatches):
      mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]
      mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]
      mini_batch = (mini_batch_X, mini_batch_Y)
      mini_batches.append(mini_batch)
  
  # Handles mini-batch whose size is not equal to ideal mini-batch size
  if m % mini_batch_size != 0:
      mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]
      mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]
      mini_batch = (mini_batch_X, mini_batch_Y)
      mini_batches.append(mini_batch)
  
  return mini_batches

def create_placeholders(n_H0, n_W0, n_C0, n_y):
  """
    Creates tensorflow placeholders

    Args:
      n_H0 = height of an image
      n_W0 = width of an image
      n_C0 = number of channels in the image
      n_y = number of classes

    Return:
      X = placeholder for input image
      Y = placeholder for output label
  """
  X = tf.placeholder(tf.float32, shape=(None, n_H0, n_W0, n_C0), name='X')
  Y = tf.placeholder(tf.float32, shape=(None, n_y), name='Y')
  
  return X, Y

def initialize_parameters():
  """
    Initializing filter values that is slided through the image

    Returns:
      Tensors of filters
  """
  tf.set_random_seed(1)                             # So that parametes randomly intialized are same every time
  
  # Initilze filters
  W1 = tf.get_variable('W1', shape=(4,4,3,8), initializer=tf.contrib.layers.xavier_initializer(seed=0))     # 8 filters of 4 × 4 × 3
  W2 = tf.get_variable('W2', shape=(2,2,8,16), initializer=tf.contrib.layers.xavier_initializer(seed=0))    # 16 filters fo 2 × 2 × 8
  
  parameters = {"W1": W1,"W2": W2}
  return parameters

def forward_propagation(X, parameters):
  """
    Implements the forward propagation for the model:
    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED
    
    Arguments:
    X -- input dataset placeholder
    parameters -- dictionary containing your parameters/filters "W1", "W2"; the shapes are given in initialize_parameters

    Returns:
    Z3 -- the output of the last LINEAR unit (SOFTMAX is not applied here)
  """

  # Retrieve filters
  W1 = parameters['W1']
  W2 = parameters['W2']
  
  # Layer 1  ==>  CONV2D -> RELU -> MAXPOOL
  Z1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME')                          # 4 × 4 × 3 filter with stride 1; padding is 'SAME'
  A1 = tf.nn.relu(Z1)                                                                     # ReLU activation
  P1 = tf.nn.max_pool(A1, ksize=[1, 8, 8, 1], strides=[1, 8, 8, 1], padding='SAME')       # 8 × 8 filter with stride 8 by 8; padding is 'SAME'
  
  # Layer 2  ==>  CONV2D -> RELU -> MAXPOOL
  Z2 = tf.nn.conv2d(P1, W2, strides=[1, 1, 1, 1], padding='SAME')                         # 2 × 2 × 8 filter with stride 1; padding is 'SAME'
  A2 = tf.nn.relu(Z2)                                                                     # ReLU activation
  P2 = tf.nn.max_pool(A2, ksize=[1, 4, 4, 1], strides=[1, 4, 4, 1], padding='SAME')       # 4 × 4 filter with stride 4 by 4; padding is 'SAME'
  
  # Layer 3  ==>  FLATTEN -> FULLYCONNECTED
  P2 = tf.contrib.layers.flatten(P2)                                                      # Flatten
  Z3 = tf.contrib.layers.fully_connected(P2, 7, activation_fn=None)                       # 7 neurons in the final layer, Softmax activation function is not applied
  return Z3

def compute_cost(Z3, Y):
  """
    Computes Cost

    Args:
      Z3 -- output of fully connected layer
      Y -- expected output for the input vector

    Returns:
      cost
  """
  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y))   # Calculates softmax
  return cost

def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009, num_epochs = 100, minibatch_size = 64, print_cost = True):
    
    ops.reset_default_graph()                         
    tf.set_random_seed(1)                             
    seed = 3                                          
    (m, n_H0, n_W0, n_C0) = X_train.shape             
    n_y = Y_train.shape[1]                            
    costs = []                                                                # Stores Cost after each epoch
    
    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)                         # Create Plaeholders
    parameters = initialize_parameters()                                      # Initialize filter values
    Z3 = forward_propagation(X, parameters)                                   # Forward Propagation of tensorflow graph
    cost = compute_cost(Z3, Y)                                                # Add Cost to tensorflow graph
    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)          # Adam Optimizer to minimize cosr
    
    init = tf.global_variables_initializer()                                  
     
    # start session
    with tf.Session() as sess:
        sess.run(init)

        # Training Loop
        for epoch in range(num_epochs):
            minibatch_cost = 0.
            num_minibatches = int(m / minibatch_size) 
            seed = seed + 1
            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)

            # Stochastic Learning : Learn from a minibatch and update values in the tensorflow graph
            for minibatch in minibatches:
                (minibatch_X, minibatch_Y) = minibatch
                _ , temp_cost = sess.run([optimizer, cost], {X: minibatch_X, Y: minibatch_Y})
                minibatch_cost += temp_cost / num_minibatches
            costs.append(minibatch_cost)

            if epoch % 5 == 0:
                print ("Cost after epoch {}: {}".format(epoch, minibatch_cost))
        
        # Plot Costs
        plt.plot(np.squeeze(costs))
        plt.ylabel('cost')
        plt.xlabel('iterations (per tens)')
        plt.title("Learning rate =" + str(learning_rate))
        plt.show()

        # Calculate the correct predictions
        predict_op = tf.argmax(Z3, 1)
        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))
        
        # Calculate accuracy on the test set
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
        print(accuracy)
        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})
        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})
        print("Train Accuracy:", train_accuracy)
        print("Test Accuracy:", test_accuracy)
                
        return train_accuracy, test_accuracy, parameters

if __name__ == '__main__':
  # root_path = '../coloured_processed_data'
  # root_path = '/Users/tanmaykalani/Documents/College/Semester 6/Deep Learning/Assignment 1/Question - 2/coloured_processed_data'
  root_path = '/Users/tanmaykalani/Desktop/S20160010096_A1/Q2_100_100_RGB_Images'
  number = 0

  os.listdir(root_path)
  X = ''
  Y = ''
  i = 0

  # Convert image to array and make INPUT FEATURE VECTORS ARRAY and EXPECTED OUTPUT ARRAY 
  for root, dir, files in os.walk(root_path):                             # Recursively walk through all the direcotries and all images within sub-directories
    for file in files:
      file_path = root + '/' + file
      print(file_path)
      cls = int((file_path.strip().split('/'))[-2])-1                     # Target Class
      image = np.array(cv2.imread(file_path))                             # Convert image to array : ( 100 × 100 × 3 ) : ( HEIGHT × WIDTH × NO_OF_CHANNELS )
      # image = np.array(matplotlib.pyplot.imread(file_path))
      print(file_path)
      if i == 0:
        i += 1
        X = np.array([image])
        Y = np.array([[int(cls)]])
      else:
        X = np.append(X, [image], axis=0)
        Y = np.append(Y, [[int(cls)]], axis=1)

  # Randomly loading dataset to TRAINING and TESTING
  X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = randomize_and_load_dataset(X, Y)

  # Printing shapes of vectors
  print('{} - SHAPE OF TRAINING AD TESTING DATASETS - {}'.format('*'*25, '*'*25))
  print('X Training : {}'.format(X_train_orig.shape))                     # 683 × 100 × 100 × 3
  print('Y Training : {}'.format(Y_train_orig.shape))                     # 1 × 683
  print('X Test : {}'.format(X_test_orig.shape))                          # 228 × 100 × 100 × 3
  print('Y Test : {}'.format(Y_test_orig.shape))                          # 1 × 228
  print('Classes Vector : {}'.format(classes.shape))                      # 7

  # Normalizing and convert vectors to appropriate formats
  X_train = X_train_orig/255.                                             # Normalize Vectors
  X_test = X_test_orig/255.                                               # Normalize Vectors
  Y_train = convert_to_one_hot(Y_train_orig, 7).T                         # One Hot Encoding
  Y_test = convert_to_one_hot(Y_test_orig, 7).T                           # One Hot Encoding
  print ("number of training examples = " + str(X_train.shape[0]))        # 683
  print ("number of test examples = " + str(X_test.shape[0]))             # 228
  print ("X_train shape: " + str(X_train.shape))                          # 683 × 100 × 100 × 3
  print ("Y_train shape: " + str(Y_train.shape))                          # 683 × 7
  print ("X_test shape: " + str(X_test.shape))                            # 228 × 100 × 100 × 3
  print ("Y_test shape: " + str(Y_test.shape))                            # 228 × 7
  conv_layers = {}

  # X, Y = create_placeholders(100, 100, 3, 7)                              
  # print ("X = " + str(X))
  # print ("Y = " + str(Y))

  # print('**'*100)

  # Training and Testing
  train, test, parameters = model(X_train, Y_train, X_test, Y_test)